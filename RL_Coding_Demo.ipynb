{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiNBNBI3q32ssVzI5RdjsD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erin-Weiss/reinforcement-learning/blob/main/RL_Coding_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Coding Demo of how to optimize a mobile robot's path in a warehouse environment with reinforcement learning**\n"
      ],
      "metadata": {
        "id": "5FuiBPFOnxOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Goal\n",
        "\n",
        "Let's consider a mobile robot in a warehouse environment that is used to retrieve items in the warehouse for packing. We want to give the robot a location of the desired item(s) and the starting place of the robot, and the robot should be able to figure out the shortest path to go and retrieve the objects. The robot must avoid obstacles in its path, or it will get a negative reward if it runs into it. To incentivize the robot to move to the desired location(s) as quickly as possible, there is also a slight negative reward for every step it takes.\n",
        "\n",
        "As we move through this demo, it is important to remember the overall Reinforcement Learning process we use to train this robot. The diagram below shows the overall process for how Reinforcement Learning works.\n",
        "\n"
      ],
      "metadata": {
        "id": "MLaQGJVYXRSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1whpRlv6tlPqjYYg1k0fmPDJC0qj4Rski\" width=\"600\"/></center>\n",
        "</div>"
      ],
      "metadata": {
        "id": "RSIJpk8_tHAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Source: https://blog.research.google/2020/04/off-policy-estimation-for-infinite.html"
      ],
      "metadata": {
        "id": "bJkelHnitQNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**–this type of path planning can also be used in other ways. For instance, in the presentation, we also talk about how this same method could be used to optimize the path of a package in a warehouse to a truck to be shipped."
      ],
      "metadata": {
        "id": "ktOE-Icceugd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why would you use Reinforcement Learning?"
      ],
      "metadata": {
        "id": "pGAcbNvlbXEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some of the reasons:\n",
        "\n",
        "- The robot does not need to be explicitly told what paths are best.\n",
        "\n",
        "- The Q-Learning Algorithm can be re-trained relatively easily with a new environment configuration (different obstacles, etc.).\n",
        "- Reinforcement Learning can continually optimize as time goes on.\n",
        "- Businesses would not necessarily need to rely on complicated and cumbersome automation strategies to do the same task.\n",
        "\n",
        "Here are some of the downsides:\n",
        "- By its nature, Reinforcement Learning is not guaranteed to find the optimal solution every time. However, it is likely that it will at least get close, and it is far faster and simpler compared to a full optimization problem.\n"
      ],
      "metadata": {
        "id": "Z3uISwrkbcN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries and Set Seed"
      ],
      "metadata": {
        "id": "p5nychSN3wUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Numpy Library\n",
        "import numpy as np\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "np.random.seed(1693)"
      ],
      "metadata": {
        "id": "DdjcNApMrzpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Environment -- Warehouse Layout Visual"
      ],
      "metadata": {
        "id": "kMqKHIa_sihc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1qCEih5nyibpyIq5A-atVpg5Pz7P_o7mS\" width=\"600\"/></center>\n",
        "</div>"
      ],
      "metadata": {
        "id": "km4jsC6v9jMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Source: Author"
      ],
      "metadata": {
        "id": "RbYZPuIptZ3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The entire environment is comprised of 100 squares with 10 rows and 10 columns.\n",
        "\n",
        "- The indices of the rows and columns start at 0—so they are indexed as 0 through 9 (as shown on the top and left of the image). We use the indices when going through the loops to determine our states later.\n",
        "- For user ease, every cell in the environment has a code name in the top right corner of each cell (example: A1, J10, etc.)\n",
        "- There are two robots in the diagram, and for this demo, the “user” would just choose which one to use based on its starting location.\n",
        "- The cells highlighted in red are obstacles that the robot cannot traverse and are given a penalty of -20, as shown at the bottom of the image.\n",
        "- In the environment, one or more cells may be highlighted in green, representing the goal(s) the robot is trying to reach. When the robot reaches the goal, it is given a reward of +100, as shown at the bottom of the image.\n",
        "- Finally, the black—or empty—cells represent the possible cells the robot can choose as part of the path that it is allowed to move along. They have values of -1 to motivate the robot to hurry up (and generate the shortest path).\n"
      ],
      "metadata": {
        "id": "jJzRq5ezsDb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Setup and Defining Functions Part 1"
      ],
      "metadata": {
        "id": "mrdDcAJ_J8hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This represents the environment rewards in an array\n",
        "rewardList = [\n",
        "[-1.,   -1.,   -1.,   -1.,   -1.,   -1.,   -1.,   -1.,  -1.,  -1.],\n",
        "[-1.,   -20.,  -20.,  -1.,   -1.,   -1.,   -1.,   -1.,  -20., -1.],\n",
        "[-1.,   -20.,  -1.,   -1.,   -1.,   -1.,   -20.,  -1.,  -1.,  -1.],\n",
        "[-1.,   -1.,   -1.,   -1.,   -1.,   -1.,   -20.,  -1.,  -20., -1.],\n",
        "[-1.,   -1.,   -1.,   -20.,  -1.,   -1.,   -20.,  -1.,  -20., -1.],\n",
        "[-1.,   -1.,   -20.,  -20.,  -1.,   -1.,   -1.,   -1.,  -20., -1.],\n",
        "[-1.,   -1.,   -1.,   -20.,  -1.,   -1.,   -1.,   -1.,  -1.,  -20.],\n",
        "[-1.,   -1.,   -1.,   -1.,   -1.,   -20.,  -1.,   -1.,  -1.,  -1.],\n",
        "[-1.,   -1.,   -20.,  -20.,  -1.,   -20.,  -1.,   -20., -20., -1.],\n",
        "[-1.,   -1.,   -1.,   -1.,   -1.,   -20.,  -1.,   -20., -20., -1.]\n",
        "]\n",
        "map_rewards=np.array(rewardList)"
      ],
      "metadata": {
        "id": "rk--oXXNJbTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** -- When we set up the initial array `map_rewards`, we do not include the goal location(s) so we can train our model to go anywhere in the environment."
      ],
      "metadata": {
        "id": "XOrWO0wh03iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create variables needed later\n",
        "actions = ['up', 'right', 'down', 'left'] # Possible actions the robot can take\n",
        "WHrows = 10     # warehouse rows\n",
        "WHcols = 10     # warehouse columns"
      ],
      "metadata": {
        "id": "-F0NjtLWgUTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a dictionary with a code name for every pair of indices for every grid cell\n",
        "# The indices are given in tuples as the key and the codename as the value\n",
        "\n",
        "keys = []\n",
        "\n",
        "for i in range(0,10):\n",
        "  for j in range(0,10):\n",
        "    keys.append((i,j))\n",
        "\n",
        "values = []\n",
        "\n",
        "for v in [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\"]:\n",
        "  for k in range(1,11):\n",
        "    value = v + str(k)\n",
        "    values.append(value)\n",
        "\n",
        "codeNames = dict(zip(keys, values))"
      ],
      "metadata": {
        "id": "nqB-sfUkruQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function finds a random legal row and column to start on while training\n",
        "\n",
        "# Important to have a variety of starting places so we do not only train our model\n",
        "# to start from one place -- we want to be able to start from anywhere\n",
        "\n",
        "def start_pos():\n",
        "    \"\"\"\n",
        "    Select starting legal position for each episode randomly\n",
        "\n",
        "    Args:\n",
        "\n",
        "    returns:\n",
        "          row(int): index of row of start position of robot\n",
        "          col(int): index of column of start position of robot\n",
        "    \"\"\"\n",
        "    row = np.random.randint(WHrows)\n",
        "    col = np.random.randint(WHcols)\n",
        "    while map_rewards[row, col] != -1.:\n",
        "        row = np.random.randint(WHrows)\n",
        "        col = np.random.randint(WHcols)\n",
        "    return row, col"
      ],
      "metadata": {
        "id": "2WYSZeY-ruDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the row/col of either the start_pos() or current state\n",
        "\n",
        "# Then generates the next action using the Epsilon Greedy Policy\n",
        "# The Epsilon Greedy Policy deals with Exploitation vs. Exploration\n",
        "# If the random number is greater than epsilon, we will do exploitation.\n",
        "# It means that the Agent will take the action with the highest value given a state.\n",
        "\n",
        "def action_next(row, col, exploration_rate):\n",
        "    \"\"\"\n",
        "    Select next action based on Epsilon Greedy Policy\n",
        "\n",
        "    Args:\n",
        "          row(int): index of row\n",
        "          col(int): index of column\n",
        "          greedy_eps: The value of epsilon\n",
        "\n",
        "    returns:\n",
        "          Action: The index of next action (ex: 'up' would be 0)\n",
        "    \"\"\"\n",
        "    if np.random.random() > exploration_rate: #Epsilon\n",
        "        return np.argmax(table_Q[row, col])\n",
        "    else: #choose a random action\n",
        "        return np.random.randint(4)"
      ],
      "metadata": {
        "id": "pQ9YRWp7ruGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function gets the new state after the action from action_next() was generated\n",
        "# Remember--the state means which cell the robot is in\n",
        "\n",
        "def location_next(row, col, action_index):\n",
        "    \"\"\"\n",
        "    Computes the indices of new row and column after taking the action\n",
        "\n",
        "    Args:\n",
        "          row(int): index of row\n",
        "          col(int): index of column\n",
        "          action_index(int): index of action\n",
        "\n",
        "    returns:\n",
        "          new_row(int): index of new row after taking the action\n",
        "          new_column(int): index of new column after taking the action\n",
        "    \"\"\"\n",
        "    new_row = row\n",
        "    new_column = col\n",
        "    if actions[action_index] == 'up' and row > 0:\n",
        "        new_row -= 1\n",
        "    elif actions[action_index] == 'right' and col < WHcols - 1:\n",
        "        new_column += 1\n",
        "    elif actions[action_index] == 'down' and row < WHrows - 1:\n",
        "        new_row += 1\n",
        "    elif actions[action_index] == 'left' and col > 0:\n",
        "        new_column -= 1\n",
        "    return new_row, new_column"
      ],
      "metadata": {
        "id": "PA6AkEo3ruIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deeper Dive Explanations\n",
        "\n",
        "This section will take a closer look at what is happening in the next two functions. The concepts below will appear later in the demo as well."
      ],
      "metadata": {
        "id": "Zp9R0ote41DR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1 - Epsilon Decay"
      ],
      "metadata": {
        "id": "7PeudBk749dV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have previously seen, when making a Reinforcement Learning model, we can have a constant epsilon for the Greedy-Epsilon Strategy. However, it may be beneficial to have a decaying epsilon strategy (meaning that it will start at 100% exploration and move to 100%--or close to it—exploitation). A decaying epsilon strategy has several advantages over a constant epsilon strategy, but the main one is that it can help the Agent converge faster to the optimal policy. However, if the decay rate is high, the Agent might get stuck if it has not explored enough environment space, so in this example, we have kept it to either .001 or .01."
      ],
      "metadata": {
        "id": "Amb4SSa8y0uQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2 - Updating Q-Table"
      ],
      "metadata": {
        "id": "TPmTiz3idapa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-Table is just a name for a simple lookup table where we calculate the maximum expected future rewards for an action at each state. The ‘Q’ stands for ‘Quality’ in Q-Table. Below is a simple illustration showing the process of creating and updating a Q-Table."
      ],
      "metadata": {
        "id": "Xlsiefufy1MU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1QI4ji9_Mcx0ky6lpARD-VyMgQML5lYxz\" width=\"300\"/></center>\n",
        "</div>"
      ],
      "metadata": {
        "id": "HIvnLhb9ACm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Source: https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial"
      ],
      "metadata": {
        "id": "gsYZA9SAACe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the Q-Table is initialized, the Q-function (seen below) is used to update it. The Q-function uses the Bellman equation and takes two inputs: state (S) and action (A). Using the equation, we can compute our Q-value for our state-action pairs. Also, note that alpha and gamma appear in this equation."
      ],
      "metadata": {
        "id": "m1yjIn8tACRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1xzPQMFqMTlbMJKJ--UUgsxuIem1mtLPE\" width=\"700\"/></center>\n",
        "</div>"
      ],
      "metadata": {
        "id": "wkabQN6KAuAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Source: https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial"
      ],
      "metadata": {
        "id": "g-QyEOgpAt1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 3 - Alpha and Gamma"
      ],
      "metadata": {
        "id": "vu95D4qNda_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alpha -\n",
        "\n",
        "- The Alpha value is the learning rate for the model, and it should be somewhere in the range of 0-1. When updating the Q-Table, the higher the learning rate, the more quickly it replaces the new q value (or the more quickly it \"learns\"). A learning rate is a tool that can be used to find how much we keep our previous knowledge of our experience that it needs to keep for our state-action pairs. This is a hyperparameter we can tune, and we will later on in this demo.\n",
        "\n",
        "Gamma -\n",
        "\n",
        "- The Gamma value is the value for future reward (or the discount rate). This value should also be somewhere in the range of 0-1, and it is also a tunable hyperparameter. If the value is equal to 1, the Agent values future rewards just as much as the current reward, so very high values for gamma might not be best for learning. Conversely, a gamma of zero will cause the Agent to value only immediate rewards, which only works well in certain cases. We will explore what gamma value works best for this model later in this demo.\n"
      ],
      "metadata": {
        "id": "c9LmQ1Ogy1qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function will train our Q-Table (initialized inside the function)\n",
        "# The Q-Table is initialized inside the function because we want a fresh Q-Table\n",
        "# every time we run the function\n",
        "\n",
        "# This function was created so we can tune our hyperparameters more easily\n",
        "\n",
        "def train(ending_cell_position):\n",
        "  \"\"\"\n",
        "  Trains the model Q-Table\n",
        "\n",
        "  Args:\n",
        "        ending_cell_position: Ending cell label (ex: 'A10')\n",
        "\n",
        "  returns:\n",
        "        mean_reward(float): the mean reward of all the episodes\n",
        "        std_reward(float): the standard deviation reward of all the episodes\n",
        "  \"\"\"\n",
        "  # This part converts the cell codename given in the argument to the row and column\n",
        "  keys_end = [k for k, v in codeNames.items() if v == ending_cell_position]\n",
        "  end_row_index, end_column_index = keys_end[0]\n",
        "\n",
        "  # This will put the reward value for the goal (100) into the part of the rewards array\n",
        "  # representing the cell given in the argument\n",
        "  global map_rewards_new\n",
        "  map_rewards_new = np.copy(map_rewards)\n",
        "  map_rewards_new[end_row_index, end_column_index] = 100\n",
        "\n",
        "  # This part initialized the Q-Table and makes sure it is accessible for other functions\n",
        "  # See Part 2 above\n",
        "  global table_Q\n",
        "  table_Q = np.zeros((WHrows, WHcols, 4))\n",
        "\n",
        "  # This empty list will hold the total rewards for each episode for the entire training\n",
        "  episode_rewards = []\n",
        "\n",
        "  # This is where the training begins\n",
        "  for episode in range(num_episodes):\n",
        "\n",
        "    # See Part 1 above on Epsilon Decay\n",
        "    exploration_rate = min_exploration_rate + \\\n",
        "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
        "\n",
        "    # Get an initial random start position for the robot\n",
        "    row_index, column_index = start_pos()\n",
        "\n",
        "    # This will count the total rewards in 1 episode\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    # This creates a loop that represents every step the robot takes within the episode\n",
        "    # It will end if the robot reaches the goal or runs into an obstacle\n",
        "    while map_rewards_new[row_index, column_index] == -1.:\n",
        "        # Gets next action based on current state\n",
        "        action_index = action_next(row_index, column_index, exploration_rate)\n",
        "\n",
        "        # Gets next state based on action from above\n",
        "        row_old, column_old = row_index, column_index\n",
        "        row_index, column_index = location_next(row_index, column_index, action_index)\n",
        "\n",
        "        # Calculates the reward for the new state\n",
        "        reward = map_rewards_new[row_index, column_index]\n",
        "\n",
        "        # Updating Q-Table (see Part 2 above)\n",
        "        # Also see Part 3 for Alpha and Gamma Explanations\n",
        "        q_old = table_Q[row_old, column_old, action_index]\n",
        "        # Temporal_difference (TD) - used to estimate the expected value of Q(St+1, a)\n",
        "        # by using the current state and action and previous state and action.\n",
        "        TD = reward + (gamma * np.max(table_Q[row_index, column_index])) - q_old\n",
        "        q_new = q_old + (alpha * TD)\n",
        "        table_Q[row_old, column_old, action_index] = q_new\n",
        "\n",
        "        # Add rewards for this step to episode total\n",
        "        total_rewards_ep += reward\n",
        "\n",
        "    # Add total rewards for this episode to list\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "\n",
        "  # Calculate the mean and std for all episodes rewards for review\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward\n"
      ],
      "metadata": {
        "id": "NvT2bey4izUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function tests our Agent with the Q-Table that was trained with the previous function\n",
        "\n",
        "def evaluate_agent(n_eval_episodes, starting_cell_position):\n",
        "  \"\"\"\n",
        "  Tests to see the performance of the Agent with the trained Q-Table\n",
        "\n",
        "  Args:\n",
        "        starting_cell_position: Starting cell label (ex: 'D4')\n",
        "        n_eval_episodes: The number of testing episodes\n",
        "\n",
        "  returns:\n",
        "        mean_reward(float): the mean reward of all the episodes\n",
        "        std_reward(float): the standard deviation reward of all the episodes\n",
        "  \"\"\"\n",
        "  # This part converts the cell codename given in the argument to the row and column\n",
        "  keys_start = [k for k, v in codeNames.items() if v == starting_cell_position]\n",
        "  start_row_index, start_column_index = keys_start[0]\n",
        "\n",
        "  # This checks if the starting cell position is valid\n",
        "  if map_rewards_new[start_row_index, start_column_index] != -1.:\n",
        "    print('Invalid starting position')\n",
        "  else:\n",
        "    # This empty list will hold the total rewards for each episode for the entire testing\n",
        "    episode_rewards = []\n",
        "\n",
        "    # This is where the testing begins\n",
        "    for episode in range(n_eval_episodes):\n",
        "\n",
        "      # Get an initial start position for the robot from start position above\n",
        "      row, col = start_row_index, start_column_index\n",
        "\n",
        "      # This will count the total rewards in 1 episode\n",
        "      total_rewards_ep = 0\n",
        "\n",
        "      # This creates a loop that represents every step the robot takes within the episode\n",
        "      # It has max_steps in case you want to limit how many steps per episode\n",
        "      for step in range(max_steps):\n",
        "\n",
        "        # It will end if the robot reaches the goal or runs into an obstacle\n",
        "        while map_rewards_new[row, col] == -1.:\n",
        "\n",
        "          # Gets next action based on current state (see note below about 0 value)\n",
        "          action_index = action_next(row, col, 0.)\n",
        "\n",
        "          # Gets next state based on action from above\n",
        "          row, col = location_next(row, col, action_index)\n",
        "\n",
        "          # Calculates the reward for the new state\n",
        "          reward = map_rewards_new[row, col]\n",
        "\n",
        "          # Add rewards for this step to episode total\n",
        "          total_rewards_ep += reward\n",
        "\n",
        "      # Add total rewards for this episode to list\n",
        "      episode_rewards.append(total_rewards_ep)\n",
        "\n",
        "    # Calculate the mean and std for all episodes rewards for review\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward\n",
        "\n"
      ],
      "metadata": {
        "id": "WE1UfR1MQpsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**--Epsilon's value is 0 here because when testing, we only want to exploit the best values and get the shortest path to optimize time spent getting the product"
      ],
      "metadata": {
        "id": "ixH9Ru1seOjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Final Functions Part 2"
      ],
      "metadata": {
        "id": "yk2-2BagsCNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When originally designing how I wanted these functions to work, I prioritized flexibility. I wanted to be able to have the robot start from anywhere and have the goal be anywhere (that is a legal space) in the environment. In order to achieve this, I had to be able to train a new Q-Table and run the trained Q-Table every time I ran the function with new starting and ending points.\n",
        "\n",
        "While this is great for user ease and makes a convenient use case, there are some drawbacks. One, it is important to make sure that we try and limit the time it takes to train and how computationally intensive the training is while making it as accurate as possible. We will address this in the hyperparameter tuning section, but it is good to keep in mind for now.\n",
        "\n",
        "Another drawback that you will see during the tuning section is consistency. Since the Q-Table has to be re-trained every time the function is run, you may get different values and, therefore, get different paths (some are shorter than others). In the tuning section, we will explore what areas we are willing to give and take on to get the desired results we want and run some tests to see how to tune our hyperparameters for consistency.\n"
      ],
      "metadata": {
        "id": "CCawYsjfLrMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function converts the states from indices to the codenames\n",
        "\n",
        "def get_codeNames_path(shortest_path):\n",
        "    \"\"\"\n",
        "    The reverse mapping from indices of locations to the code names is done.\n",
        "\n",
        "    Args:\n",
        "          shortest_path(List): its a list of tuples containing indices\n",
        "\n",
        "    returns:\n",
        "          pathList: List of locations in final path in the form of code names\n",
        "    \"\"\"\n",
        "    pathList = []\n",
        "    for i in shortest_path:\n",
        "        j=tuple(i)\n",
        "        pathList.append(codeNames[j])\n",
        "    return pathList"
      ],
      "metadata": {
        "id": "xA_qO4ITruLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function brings everything together that we have learned so far\n",
        "# Almost all of it we have seen in other functions, so I will not comment every line\n",
        "\n",
        "def final_path(starting_cell_position, ending_cell_position):\n",
        "    \"\"\"\n",
        "    Computes/prints final path of Agent/Robot\n",
        "\n",
        "    Args:\n",
        "          starting_cell_position: Starting cell label (ex: 'D4')\n",
        "          ending_cell_position: Ending cell label (ex: 'A10')\n",
        "\n",
        "    returns:\n",
        "          pathList(NoneType): List of locations in final path in the form of code names\n",
        "    \"\"\"\n",
        "    keys_start = [k for k, v in codeNames.items() if v == starting_cell_position]\n",
        "    start_row_index, start_column_index = keys_start[0]\n",
        "\n",
        "    keys_end = [k for k, v in codeNames.items() if v == ending_cell_position]\n",
        "    end_row_index, end_column_index = keys_end[0]\n",
        "\n",
        "    global map_rewards_new\n",
        "    map_rewards_new = np.copy(map_rewards)\n",
        "    map_rewards_new[end_row_index, end_column_index] = 100\n",
        "\n",
        "    global table_Q\n",
        "\n",
        "    table_Q = np.zeros((WHrows, WHcols, 4))\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "      exploration_rate = min_exploration_rate + \\\n",
        "      (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
        "\n",
        "      row_index, column_index = start_pos()\n",
        "\n",
        "      while map_rewards_new[row_index, column_index] == -1.:\n",
        "        action_index = action_next(row_index, column_index, exploration_rate)\n",
        "\n",
        "        row_old, column_old = row_index, column_index\n",
        "        row_index, column_index = location_next(row_index, column_index, action_index)\n",
        "\n",
        "        reward = map_rewards_new[row_index, column_index]\n",
        "\n",
        "        q_old = table_Q[row_old, column_old, action_index]\n",
        "        # temporal_difference\n",
        "        TD = reward + (gamma * np.max(table_Q[row_index, column_index])) - q_old\n",
        "        q_new = q_old + (alpha * TD)\n",
        "        table_Q[row_old, column_old, action_index] = q_new\n",
        "\n",
        "    # Creates a list object for the suggested path\n",
        "    pathList=[]\n",
        "    if map_rewards_new[start_row_index, start_column_index] != -1.:\n",
        "        print('Invalid starting position')\n",
        "    else:\n",
        "        row, col = start_row_index, start_column_index\n",
        "        # This holds the row/col list of the suggested path\n",
        "        shortest_path = []\n",
        "        shortest_path.append([row, col])\n",
        "        while map_rewards_new[row, col] == -1.:\n",
        "            action_index = action_next(row, col, 0.)\n",
        "            row, col = location_next(row, col, action_index)\n",
        "            shortest_path.append([row, col])\n",
        "            # This converts the rows/cols into the codenames for user ease\n",
        "            pathList=get_codeNames_path(shortest_path)\n",
        "        # This creates a nice looking print statement\n",
        "        for i in pathList:\n",
        "          if i != pathList[-1]:\n",
        "            print(i, end =\" => \")\n",
        "          else:\n",
        "            print(i)"
      ],
      "metadata": {
        "id": "rMMIHzJ3ruAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is almost identical to the previous--it only changes the return statement to a list\n",
        "# We want the return statement to be a list so we can concatenate two later\n",
        "\n",
        "# Almost all of it we have seen in other functions, so I will not comment every line\n",
        "\n",
        "def pre_extra_stop(starting_cell_position, ending_cell_position):\n",
        "    \"\"\"\n",
        "    Computes/prints final path of Agent/Robot (used within extra stop function)\n",
        "\n",
        "    Args:\n",
        "          starting_cell_position: Starting cell label (ex: 'D4')\n",
        "          ending_cell_position: Ending cell label (ex: 'A10')\n",
        "\n",
        "\n",
        "    returns:\n",
        "          pathList(List): List of locations in final path in the form of code names\n",
        "    \"\"\"\n",
        "    keys_start = [k for k, v in codeNames.items() if v == starting_cell_position]\n",
        "    start_row_index, start_column_index = keys_start[0]\n",
        "\n",
        "    keys_end = [k for k, v in codeNames.items() if v == ending_cell_position]\n",
        "    end_row_index, end_column_index = keys_end[0]\n",
        "\n",
        "    global map_rewards_new\n",
        "    map_rewards_new = np.copy(map_rewards)\n",
        "    map_rewards_new[end_row_index, end_column_index] = 100\n",
        "\n",
        "    global table_Q\n",
        "\n",
        "    table_Q = np.zeros((WHrows, WHcols, 4))\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "      exploration_rate = min_exploration_rate + \\\n",
        "      (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
        "      row_index, column_index = start_pos()\n",
        "\n",
        "      while map_rewards_new[row_index, column_index] == -1.:\n",
        "        action_index = action_next(row_index, column_index, exploration_rate)\n",
        "\n",
        "        row_old, column_old = row_index, column_index\n",
        "        row_index, column_index = location_next(row_index, column_index, action_index)\n",
        "\n",
        "        reward = map_rewards_new[row_index, column_index]\n",
        "        q_old = table_Q[row_old, column_old, action_index]\n",
        "        # temporal_difference\n",
        "        TD = reward + (gamma * np.max(table_Q[row_index, column_index])) - q_old\n",
        "        q_new = q_old + (alpha * TD)\n",
        "        table_Q[row_old, column_old, action_index] = q_new\n",
        "\n",
        "    pathList=[]\n",
        "    if map_rewards_new[start_row_index, start_column_index] != -1.:\n",
        "        print('Invalid starting position')\n",
        "    else:\n",
        "        row, col = start_row_index, start_column_index\n",
        "        shortest_path = []\n",
        "        shortest_path.append([row, col])\n",
        "        while map_rewards_new[row, col] == -1.:\n",
        "            action_index = action_next(row, col, 0.)\n",
        "            row, col = location_next(row, col, action_index)\n",
        "            shortest_path.append([row, col])\n",
        "            pathList=get_codeNames_path(shortest_path)\n",
        "        # This is where we just return the list\n",
        "        return pathList"
      ],
      "metadata": {
        "id": "mKw8o-CxxRS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function allows for one more argument of a middle stop in the path\n",
        "\n",
        "# We use the previous function so we can concatenate the two lists together to make\n",
        "# a final path\n",
        "\n",
        "def extra_stop(starting_location, intermediary_location, ending_location):\n",
        "    \"\"\"\n",
        "    Computes/prints final path of Agent/Robot with extra stop\n",
        "\n",
        "    Args:\n",
        "          starting_cell_position: Starting cell label (ex: 'D4')\n",
        "          intermediary_location: intermediary cell label (ex: 'D6')\n",
        "          ending_cell_position: Ending cell label (ex: 'A10')\n",
        "\n",
        "    returns:\n",
        "          pathList(NoneType): List of locations in final path in the form of code names\n",
        "    \"\"\"\n",
        "    # Here we run the pre_extra_stop() function twice with the intermediary_location\n",
        "    # in the middle. We also use [1:] so the intermediary_location is not listed twice\n",
        "    final = pre_extra_stop(starting_location, intermediary_location) + pre_extra_stop(intermediary_location, ending_location)[1:]\n",
        "    for i in final:\n",
        "      if i != final[-1]:\n",
        "        print(i, end =\" => \")\n",
        "      else:\n",
        "        print(i)"
      ],
      "metadata": {
        "id": "bexl3VPesnu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model and Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "K70qr3hZIBX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will tune our hyperparameters to see what values work best for this model in training and testing. This is especially important because (as I mentioned earlier) when we run the final function, we will train a new Q-Table and run the newly trained Q-Table every time.\n",
        "\n",
        "In order to try and fully discover how this affects our model, we will be exploring the hyperparameter tuning in two sections—one that will focus on how each parameter affects the training numbers of the model. The other section will focus on consistency when running the testing functions with the same inputs many times with slightly different Q-Tables each time.\n",
        "\n",
        "We will pay special attention to the hyperparameters num_episodes, alpha, exploration_decay_rate, and gamma. Please reference the explanations above to understand why we would want to tune alpha, exploration_decay_rate, and gamma. We want to tune num_episodes because that is one of the main metrics we can control that affects how long it takes to train our model and, therefore, how long it will take to run our final function. When we get to the consistency section, we have to weigh the pros and cons of the value of a quick vs. accurate function."
      ],
      "metadata": {
        "id": "bdZr0LK-jiMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seeing how Hyperparameters affect model"
      ],
      "metadata": {
        "id": "vhZwezcTLEkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are two parameters that could be tuned for the testing\n",
        "\n",
        "# This represents the number of episodes it will train\n",
        "n_eval_episodes = 100\n",
        "\n",
        "# This represents the max steps the robot can take in 1 episode\n",
        "# This is not needed as much for this demo, but it could be useful if it was larger\n",
        "max_steps = 100"
      ],
      "metadata": {
        "id": "xfqv3G5BzsuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**--even though we are mostly paying attention to the training values at this point, it is still helpful to see how it performs in testing."
      ],
      "metadata": {
        "id": "UA-69C9IQdL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test 1 (a)\n",
        "\n",
        "- Initial test (nothing to change)"
      ],
      "metadata": {
        "id": "n2bKPONWFEcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the main hyperparameters we will be tuning\n",
        "# In particular, num_episodes, alpha, exploration_decay_rate, and gamma\n",
        "\n",
        "num_episodes = 5000\n",
        "\n",
        "alpha = 0.1 # learning_rate\n",
        "gamma = 0.99 # discount_rate\n",
        "\n",
        "exploration_rate = 1 # epsilon\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001 # decay rate\n"
      ],
      "metadata": {
        "id": "Cfju5EAoH2pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train('E10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr72_oLrldsO",
        "outputId": "3b523524-c8c6-429f-ebb5-d264a34f4070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48.941, 56.095517815597354)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_agent(n_eval_episodes, 'H1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvxuSse0H2iZ",
        "outputId": "11c273ce-1202-454d-f6e0-7098ba45dd07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(83.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What did we learn?"
      ],
      "metadata": {
        "id": "POjEoR65RNSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a good baseline to see how the parameters affect the model when changed."
      ],
      "metadata": {
        "id": "2_7NbbZxRO6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test 2 (a)\n",
        "\n",
        "- What did we change?\n",
        "  - num_episodes from 5000 to 10000"
      ],
      "metadata": {
        "id": "R2Fonn8oFJvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 10000\n",
        "\n",
        "alpha = 0.1 # learning_rate\n",
        "gamma = 0.99 # discount_rate\n",
        "\n",
        "exploration_rate = 1 # epsilon\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001 # decay rate"
      ],
      "metadata": {
        "id": "SSCvuZHvH2fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train('E10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zTGPI2IH2b7",
        "outputId": "d731487f-8e78-4fbb-8737-b854dd9ac711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67.1316, 46.71011754898504)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_agent(n_eval_episodes, 'H1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFBoI93wH2ZD",
        "outputId": "51c0b767-615b-4e86-cf0a-127e9de896b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1J8fZcv6SFqmvt2UYaFGqn6SXE3k1BSFY\" width=\"600\"/></center>\n",
        "</div>"
      ],
      "metadata": {
        "id": "r5tyJJl2nwwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**--The 85 we see above is 100-15, which represents the reward of 100 subtracted by the -1 * 15 squares it took to get there (it does not count the first square). As a spot check, this makes sense. Given that this is the most efficient number (so far), we want to get as close as possible to this when tuning in training--without getting too close because we want the model to explore enough in the beginning."
      ],
      "metadata": {
        "id": "QigQbbkypqit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What did we learn?"
      ],
      "metadata": {
        "id": "zEwn1eC5GytB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the num_episodes definitely increases the overall accuracy of the training (and testing--85 is better than 83). However, this takes more time than a lower number, and it is possible that we do not need all that training for the testing to be accurate. We can also potentially get accurate training numbers by tuning the other hyperparameters. One potential reason the accuracy likely increases so much is that, towards a larger portion of the end, the epsilon rate is exploiting more. We will continue to explore and see what we find.\n"
      ],
      "metadata": {
        "id": "TXwYUUFBjmMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test 3 (a)\n",
        "\n",
        "- What did we change?\n",
        "  - num_episodes from 10000 to 1000\n",
        "  - exploration_decay_rate from 0.001 to 0.01 (the reason is that the function gets hung up with such a \"small\" number of episodes at the smaller decay rate)\n",
        "  - from here on, we will keep the decay rate at 0.01 until the next section"
      ],
      "metadata": {
        "id": "N5OVJu3DFMPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 1000\n",
        "\n",
        "alpha = 0.1 # learning_rate\n",
        "gamma = 0.99 # discount_rate\n",
        "\n",
        "exploration_rate = 1 # epsilon\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.01 # decay rate"
      ],
      "metadata": {
        "id": "ByAGxXSIpfe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train('E10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDvsRZVnpfbU",
        "outputId": "c93d96e0-1f54-4558-f5ec-49f31c42317a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54.227, 57.52407731550329)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_agent(n_eval_episodes, 'H1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuBqdS_mpfYe",
        "outputId": "e28094b0-4fd4-43e1-bfc0-67b5c944bb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What did we learn?"
      ],
      "metadata": {
        "id": "dgAujvPLG3wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decreasing the num_episodes to 1000 decreases the accuracy of the training, but it also decreases the time the function takes to run. Also, the testing was still the best version so far at this number, so it may end up being worth it to keep the training episodes lower, especially after we tune the other hyperparameters.\n",
        "\n",
        "It is also worth noting that we had to make the epsilon decay rate larger when the num_episodes is smaller because otherwise, the functions get stuck. This makes sense because there are fewer episodes for epsilon to decay. The same is true that the larger decay rate does not work as well in this case with the larger number of episodes.\n"
      ],
      "metadata": {
        "id": "WodI7CIbkmld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test 4 (a)\n",
        "\n",
        "- What did we change?\n",
        "  - gamma from 0.99 to 0.7\n"
      ],
      "metadata": {
        "id": "6v7fq6JQFOHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 1000\n",
        "\n",
        "alpha = 0.1 # learning_rate\n",
        "gamma = 0.7 # discount_rate\n",
        "\n",
        "exploration_rate = 1 # epsilon\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.01 # decay rate"
      ],
      "metadata": {
        "id": "Csk5uDL_pfVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train('E10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eu1yKSnpfPS",
        "outputId": "6cbc3378-aa6f-4311-f071-4e1147651c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50.228, 60.13365127779952)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_agent(n_eval_episodes, 'H1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBkE5k_CtFiQ",
        "outputId": "97a516ee-84a8-41be-d1ee-868506bcbce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What did we learn?"
      ],
      "metadata": {
        "id": "uEDnkAftG45A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on this test, we should try and keep gamma as high as possible."
      ],
      "metadata": {
        "id": "74QrNRHOSu9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test 5 (a)\n",
        "\n",
        "- What did we change?\n",
        "  - alpha from 0.1 to 0.5"
      ],
      "metadata": {
        "id": "7ROjelAwFP14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 1000\n",
        "\n",
        "alpha = 0.5 # learning_rate\n",
        "gamma = 0.99 # discount_rate\n",
        "\n",
        "exploration_rate = 1 # epsilon\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.01 # decay rate"
      ],
      "metadata": {
        "id": "aCVwnsz3wKb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train('E10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKyqaFagwNnf",
        "outputId": "107f77f4-2227-4dbf-f34b-7682ceb4bc1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59.909, 52.76279673216725)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_agent(n_eval_episodes, 'H1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjYc_jSAwNkc",
        "outputId": "2d902cbf-46fb-463c-b99d-ff26f5785000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What did we learn?"
      ],
      "metadata": {
        "id": "C4A6QGaUG6bE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this test, we learned that compared to Test 3, it is better if the alpha is higher."
      ],
      "metadata": {
        "id": "f-kZGsiATAmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test 6 (a)\n",
        "\n",
        "- What did we change?\n",
        "  - gamma from 0.5 to 0.99"
      ],
      "metadata": {
        "id": "c0UTum1gFRdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 1000\n",
        "\n",
        "alpha = 0.99 # learning_rate\n",
        "gamma = 0.99 # discount_rate\n",
        "\n",
        "exploration_rate = 1 # epsilon\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.01 # decay rate"
      ],
      "metadata": {
        "id": "w4El4ghMwNhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train('E10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcV3MQDBwbIl",
        "outputId": "c350a198-afbe-4fe5-ffa0-dfcc2feffc01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(61.022, 51.91371221556016)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_agent(n_eval_episodes, 'H1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8bRh6uawbEo",
        "outputId": "f0386379-16c0-40a3-9755-06680df77b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What did we learn?"
      ],
      "metadata": {
        "id": "vrSd-ZvuG7oS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this final test in this section, we confirm that the model performs fairly well when alpha is high. We will explore more in the next section."
      ],
      "metadata": {
        "id": "xUrhB9gzTx5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring Consistency"
      ],
      "metadata": {
        "id": "NzvMavZ1LVam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will be exploring the distribution of path lengths given by running the function `pre_extra_stop()` 100 times. Our goal is to have the majority of responses be the shortest path and have the fewest number of different paths suggested."
      ],
      "metadata": {
        "id": "7yxWwWHNLZ7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining Function and setting up for tests"
      ],
      "metadata": {
        "id": "JDXh1A2OUdPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of times we want to run our final function\n",
        "\n",
        "num_tests = 100"
      ],
      "metadata": {
        "id": "I3p-FNz0ORvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the input of num_tests to check the path length each time\n",
        "# the final function pre_extra_stop is run (I chose that function since you can count in lists)\n",
        "\n",
        "# The dictionary then sorts and counts each type of length that comes from function\n",
        "# pre_extra_stop during the tests\n",
        "\n",
        "def my_test_function(num_tests, starting_cell_position, ending_cell_position):\n",
        "  \"\"\"\n",
        "    Checks the distribution of suggested path lengths by running pre_extra_stop()\n",
        "    num_tests amount of times\n",
        "\n",
        "    Args:\n",
        "          num_tests: Number of times to run pre_extra_stop() function\n",
        "          starting_cell_position: Starting cell label (ex: 'D4')\n",
        "          ending_cell_position: Ending cell label (ex: 'A10')\n",
        "\n",
        "    returns:\n",
        "          dict(dictionary): Dictionary of path lengths with lengths as key and\n",
        "                            frequency as value\n",
        "  \"\"\"\n",
        "  list = []\n",
        "\n",
        "  for i in range(num_tests+1):\n",
        "    list.append(len(pre_extra_stop(starting_cell_position, ending_cell_position)))\n",
        "\n",
        "  dict = {i:list.count(i) for i in list}\n",
        "\n",
        "  return dict"
      ],
      "metadata": {
        "id": "J6kogh2OOR4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**--The function `my_test_function()` can take a while to run depending on num_episodes"
      ],
      "metadata": {
        "id": "TnVhh5cTWniL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**--The numbers I mention in the subsequent sections may be slightly off due to the nature of testing. However, the overall ideas should still be accurate regardless."
      ],
      "metadata": {
        "id": "H9ftZTyyjpPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Running Tests on Consistency"
      ],
      "metadata": {
        "id": "ntAeGXk2UtTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test 1 (b)\n",
        "\n",
        "- We are going to start with where we left off in the previous section"
      ],
      "metadata": {
        "id": "APZ_Z7ygWONS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 1000\n",
        "\n",
        "alpha = 0.99 # learning_rate\n",
        "gamma = 0.99 # discount_rate\n",
        "\n",
        "exploration_rate = 1 # epsilon\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.01 # decay rate"
      ],
      "metadata": {
        "id": "SUL37RK-Uw2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_test_function(num_tests, 'H1', 'E10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ounulwbXUxbz",
        "outputId": "f12e9fc9-ed7d-4095-ebe7-b61bcadf7287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{19: 38, 17: 60, 21: 3}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What did we learn?"
      ],
      "metadata": {
        "id": "962ONW97W4Mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we start to see the problem. In the 100 times the `pre_extra_stop()` function was run, it only got the actual shortest path 56 of those times. The other path lengths are close, but ideally, we would want a more likely chance of getting the shortest path every time we run the function."
      ],
      "metadata": {
        "id": "HuWnc5O3XDCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test 2 (b)\n",
        "\n",
        "- What did we change?\n",
        "  - We increased the `num_episodes` back to 10000\n",
        "  - We changed the decay rate back to 0.001"
      ],
      "metadata": {
        "id": "5qXoWrYyYGaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 10000\n",
        "\n",
        "alpha = 0.99 # learning_rate\n",
        "gamma = 0.99 # discount_rate\n",
        "\n",
        "exploration_rate = 1 # epsilon\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001 # decay rate"
      ],
      "metadata": {
        "id": "uviCVc7vUxRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_test_function(num_tests, 'H1', 'E10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qNg0cVvUxMv",
        "outputId": "3592fe2a-db50-414c-88ed-556e93f2783b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{17: 93, 19: 8}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What did we learn?"
      ],
      "metadata": {
        "id": "Wn6lwTl4YH96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this test, we learned that it performed much better when there were more episodes in testing. There were fewer path options, and it got the shortest path 91 out of 100 times. However, this comes at the expense of a speedy function later on. Let’s see if we can get an almost as good version that would be faster."
      ],
      "metadata": {
        "id": "eSaSqL4ecl1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test 3 (b)\n",
        "\n",
        "- What did we change?\n",
        "  - We dropped the `num_episodes` down to 2500 to try and speed up the function\n",
        "  - We changed the decay rate to 0.01"
      ],
      "metadata": {
        "id": "DttdY0WtYIXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 2500\n",
        "\n",
        "alpha = 0.99 # learning_rate\n",
        "gamma = 0.99 # discount_rate\n",
        "\n",
        "exploration_rate = 1 # epsilon\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.01 # decay rate"
      ],
      "metadata": {
        "id": "GePp5bnzYJNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_test_function(num_tests, 'H1', 'E10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_CThUwpYJr3",
        "outputId": "032cd61f-44e0-464b-e5a8-63c9781af4df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{17: 72, 19: 26, 21: 3}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What did we learn?"
      ],
      "metadata": {
        "id": "4CWbzoXPa7ii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test performed slightly better than Test 1 (b), but not by a large margin. Plus, it performed much worse than the 10,000 episodes test. However, it would run faster, so let’s run another test to see if we can get 2500 to improve."
      ],
      "metadata": {
        "id": "_ifV3lcta7ON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test 4 (b)\n",
        "\n",
        "- What did we change?\n",
        "  - We changed the decay rate back to 0.001"
      ],
      "metadata": {
        "id": "gSPaLajJa7KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 2500\n",
        "\n",
        "alpha = 0.99 # learning_rate\n",
        "gamma = 0.99 # discount_rate\n",
        "\n",
        "exploration_rate = 1 # epsilon\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001 # decay rate"
      ],
      "metadata": {
        "id": "XrSOKkOxYJj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_test_function(num_tests, 'H1', 'E10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf5YlIercIhs",
        "outputId": "44ffb326-cad6-41b1-cc41-222bb78a2033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{17: 82, 19: 19}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What did we learn?"
      ],
      "metadata": {
        "id": "9sDMTy1Ha7de"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decreasing the decay rate really helped in this case. The final breakdown was that it got the shortest path 84 out of the 100 times. While this is not quite as good as the 10,000 episodes, this will take a lot shorter to run when used in the end."
      ],
      "metadata": {
        "id": "7_VJKD8oa7FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the Functions and Visuals"
      ],
      "metadata": {
        "id": "IqAuBz9psqCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the best versions of the hyperparameters, given the tradeoffs."
      ],
      "metadata": {
        "id": "KCFQJFJ4UIV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the tuned hyperparameters that were decided earlier\n",
        "\n",
        "num_episodes = 2500\n",
        "\n",
        "alpha = 0.99 # learning_rate\n",
        "gamma = 0.99 # discount_rate\n",
        "\n",
        "exploration_rate = 1 # epsilon\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001 # decay rate"
      ],
      "metadata": {
        "id": "LR_RDXVIeKah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does this tie back into our problem?"
      ],
      "metadata": {
        "id": "P7RNnvei-ZU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we remember our Case Study from the Presentation, making a system similar to this one could help companies like Stitch Fix (or Amazon) find the optimal pathways for their robots to collect items for packing. The goal of finding the shortest path is to reduce time and increase efficiency so they can do more business. Also, having autonomous robots able to find pathways through a warehouse (despite obstacles) can free up people to do other tasks or their tasks more efficiently."
      ],
      "metadata": {
        "id": "vMZSJDDw-d8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**--Since there are multiple valid solutions sometimes the solution the code runs and the picture may not be the same. However, the pictures show a path that the code has come up with before."
      ],
      "metadata": {
        "id": "CN9AJsBCuAmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here is what it looks like when running the function!"
      ],
      "metadata": {
        "id": "MoD0AEAxufS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_path('D4', 'A10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt7V_VcWssdP",
        "outputId": "d04bc9a1-5b9a-44e9-cf5c-3511522110ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D4 => C4 => B4 => A4 => A5 => A6 => A7 => A8 => A9 => A10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1Td_swsD3G3dMpFjje5YJreaXJhdn1iYo\" width=\"600\"/></center>\n",
        "</div>"
      ],
      "metadata": {
        "id": "ZFeUjfHc4ShK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Source: Author"
      ],
      "metadata": {
        "id": "czotZgEGFVeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_path('D1', 'E10')"
      ],
      "metadata": {
        "id": "r0OSQYQqv6So",
        "outputId": "d10beccc-ee2e-4281-a269-279e32332e87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D1 => D2 => D3 => C3 => C4 => B4 => B5 => B6 => B7 => B8 => C8 => C9 => C10 => D10 => E10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1QG72bNEc3fKuUOC--9ad8bdeFeXR_35K\" width=\"600\"/></center>\n",
        "</div>"
      ],
      "metadata": {
        "id": "dSrOFMV24TCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Source: Author"
      ],
      "metadata": {
        "id": "SnTZzmrPFXpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extra_stop('D4', 'D6', 'A10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0d02Eqxssxn",
        "outputId": "943487fb-b8ac-4f99-8d51-1248ef9a530b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D4 => D5 => D6 => C6 => B6 => A6 => A7 => A8 => A9 => A10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1W4we3cwgW7NaXfWtL4Qo1TkbUyT7kIqR\" width=\"600\"/></center>\n",
        "</div>"
      ],
      "metadata": {
        "id": "P9Q_zbAp4ToB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Source: Author"
      ],
      "metadata": {
        "id": "7cqpYo2zFZKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_path('H1', 'E10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6foyXtwzAQ8",
        "outputId": "f371446f-3c9c-489c-bf12-99d136c8a38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H1 => H2 => H3 => H4 => H5 => G5 => F5 => F6 => F7 => F8 => E8 => D8 => C8 => C9 => C10 => D10 => E10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1J8fZcv6SFqmvt2UYaFGqn6SXE3k1BSFY\" width=\"600\"/></center>\n",
        "</div>"
      ],
      "metadata": {
        "id": "AkLd6V6q4Ubz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Source: Author"
      ],
      "metadata": {
        "id": "OrYAaxdiFcFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extra_stop('H1', 'H7', 'E10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek-8OwR6zAN7",
        "outputId": "1b29caa5-a8c3-459f-9a62-ac2759c4acb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H1 => H2 => H3 => H4 => H5 => G5 => G6 => G7 => H7 => G7 => F7 => F8 => E8 => D8 => C8 => C9 => C10 => D10 => E10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1gShxU5VSGL00JGo5-hvcebCrFh0LVeM0\" width=\"600\"/></center>\n",
        "</div>"
      ],
      "metadata": {
        "id": "AnWnoQnc4Uua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Source: Author"
      ],
      "metadata": {
        "id": "TwtZYPzdFeGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Sources for this Demo (see presentation for full citation list):"
      ],
      "metadata": {
        "id": "Dabii-VG7alf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Reinforcement Learning 8: Pick and Place Robot in an E-Commerce Store Warehouse i.e. Q-Learning in Action](https://ashutoshmakone.medium.com/reinforcement-learning-8-pick-and-place-robot-in-an-e-commerce-store-warehouse-i-e-78d7af7e60c8)\n",
        "\n",
        "[Get Started With Q-Learning With Python: How To Automatize A Warehouse Robot](https://medium.datadriveninvestor.com/get-started-with-q-learning-with-python-how-to-automatize-a-warehouse-robot-7bfae0180301)\n",
        "\n",
        "[An Introduction to Q-Learning: A Tutorial For Beginners](https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial)"
      ],
      "metadata": {
        "id": "0tRC9DvJ7d_3"
      }
    }
  ]
}