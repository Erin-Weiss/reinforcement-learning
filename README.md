# ü§ñ Warehouse Robot Path Optimization using Reinforcement Learning (Google Colab Demo)  
**Author:** Erin Weiss  

---

## üß† Coding Demo

üëâ [**Click here to open the full Google Colab notebook**](https://colab.research.google.com/github/Erin-Weiss/reinforcement-learning/blob/main/copy_of_coding_demo.ipynb)

---

## üìä Project Overview  
This Google Colab project demonstrates how reinforcement learning‚Äîspecifically **Q-Learning**‚Äîcan be used to optimize a **mobile robot‚Äôs path** in a warehouse environment.  
The goal is for the robot to navigate from a starting location to a target cell while avoiding obstacles and minimizing travel steps. The model uses **rewards and penalties** to learn the most efficient routes dynamically.  

This work highlights how reinforcement learning can streamline warehouse operations, reduce travel time, and increase fulfillment efficiency for companies such as Amazon or Stitch Fix.  

---

## üèÅ About This Project  
This project demonstrates my ability to:  
- Design, train, and evaluate a **reinforcement learning agent** using Q-Learning  
- Implement **dynamic environment mapping** with adjustable goals and obstacles  
- Tune **hyperparameters** to balance accuracy, speed, and consistency  
- Translate a machine learning model into a clear, educational **Colab demo** with visual explanations  

---

## üß© Features & Functionality  
**Dynamic Path Planning** ‚Äì The agent learns the optimal route in a 10√ó10 warehouse grid.  
**Obstacle Avoidance** ‚Äì Cells with penalties simulate shelves or barriers in a real warehouse.  
**Flexible Start/Goal Selection** ‚Äì Any valid cell can serve as the starting or goal position.  
**Q-Table Training** ‚Äì Uses the Bellman equation to iteratively improve action‚Äìstate values.  
**Hyperparameter Tuning** ‚Äì Adjusts Œ± (learning rate), Œ≥ (discount factor), and Œµ (exploration rate).  
**Extra Stop Functionality** ‚Äì Includes an intermediary stop option to simulate multi-point retrieval.  
**Consistency Testing** ‚Äì Evaluates stability by running 100 independent training iterations.  

---

## üß∞ Technologies & Tools Used  
- **Google Colab / Python 3** ‚Äì Implementation and interactive visualization  
- **NumPy** ‚Äì Matrix operations and environment simulation  
- **Matplotlib / Inline Visuals** ‚Äì Grid visualizations of robot paths  
- **Reinforcement Learning (Q-Learning Algorithm)** ‚Äì Path optimization method  
- **Markdown Documentation** ‚Äì Embedded technical explanations within Colab  

---

## ‚öôÔ∏è Project Details  
| Parameter | Description |
|------------|-------------|
| **Environment Size** | 10 √ó 10 grid (100 cells) |
| **Rewards** | +100 (goal), ‚Äì20 (obstacle), ‚Äì1 (step) |
| **Episodes** | 2,500 (default, tuned for accuracy vs speed) |
| **Learning Rate (Œ±)** | 0.99 |
| **Discount Factor (Œ≥)** | 0.99 |
| **Exploration Decay Rate** | 0.001 |
| **Epsilon (Œµ)** | Starts at 1.0 ‚Üí decays to 0.01 |
| **File Type** | Python (.ipynb) Google Colab Notebook |

---

## üí° Key Insights  
- Increasing **training episodes** improves consistency but raises computation time.  
- A **higher learning rate** (Œ± ‚âà 0.99) accelerates convergence.  
- Lower **decay rates** encourage sufficient exploration before exploitation.  
- The robot reliably achieves the **shortest path ‚âà 84 %** of the time at 2,500 episodes.  
- Reinforcement learning enables **adaptive pathfinding** without explicit rule-based programming.

> **Note:** Because the model retrains from scratch each time the notebook is run, specific numerical results (such as rewards or episode averages) may vary slightly between runs.  
> Reported values and outputs represent *typical results* that are close but not always identical.

---

## üß† Analysis & Commentary  
This demonstration illustrates how Q-Learning can model real-world warehouse optimization.  
Each grid cell represents a discrete state, and the robot learns through iterative exploration to maximize cumulative reward.  

- **Epsilon Decay Strategy:** Balances exploration and exploitation to avoid local optima.  
- **Alpha (Learning Rate):** Controls how quickly the model updates knowledge from new experiences.  
- **Gamma (Discount Rate):** Determines the importance of future vs immediate rewards.  
- **Q-Table Updates:** Use the Bellman equation to iteratively refine expected outcomes.  

The results show that after training, the agent can **consistently select the most efficient route**, demonstrating reinforcement learning‚Äôs practical potential for logistics, robotics, and autonomous navigation systems.  

---

## üßæ Example Outputs  

```python
final_path('D4', 'A10')
# Output:
D4 => C4 => B4 => A4 => A5 => A6 => A7 => A8 => A9 => A10

extra_stop('D4', 'D6', 'A10')
# Output:
D4 => D5 => D6 => C6 => B6 => A6 => A7 => A8 => A9 => A10
```

## üß≠ Business Context  
Companies with large warehouse operations (like **Amazon**, **Ocado**, or **Stitch Fix**) use similar algorithms to optimize fulfillment routes.  
Reinforcement learning enables robots to:

- **Self-adapt** to new obstacles and layout changes  
- **Retrain quickly** with minimal human intervention  
- **Increase throughput efficiency** while minimizing operational costs  
- **Scale automation** without requiring new rule-based logic  

These advantages make Q-Learning a practical foundation for developing real-world **autonomous warehouse systems** that combine flexibility, efficiency, and intelligence.  

---

## üìÅ Repository Contents  

| File | Description |
|------|-------------|
| `copy_of_coding_demo.ipynb` | Full Google Colab notebook with Q-Learning implementation |
| `warehouse-grid.png` | Environment layout visualization |
| `README.md` | Project documentation and overview |

---

## üîó Links  

- **Live Colab Notebook:** [Open in Google Colab](https://colab.research.google.com/github/Erin-Weiss/reinforcement-learning/blob/main/copy_of_coding_demo.ipynb)
- **GitHub Repository:** [View Source Code](https://github.com/Erin-Weiss/reinforcement-learning)
- **Portfolio Page:** [View on Portfolio](https://erin-weiss.github.io/articles/RL-Article.html)

